# Reinforcement learning using openai/gym

import os
import json
import torch
import torch.nn as nn
import torch.optim as optim
import numpy as np
import matplotlib.pyplot as plt
from torch.distributions import Categorical
from gym import Env, spaces
from stable_baselines3 import PPO
from stable_baselines3.common.torch_layers import BaseFeaturesExtractor
from stable_baselines3 import PPO
from random import choice
import torch.nn.functional as F


# Hyperparameters
EPOCHS = 50
SAVE_INTERVAL = 5
LEARNING_RATE = 0.0005
GAMMA = 0.99
GAE_LAMBDA = 0.95
PPO_EPOCHS = 4
MINI_BATCH_SIZE = 64
PPO_CLIP = 0.2
TARGET_KL = 0.01
MAX_STEPS = 100

# Directories
preprocessed_data_directory = "D:/Github/dungeonCrawlerMgr/LevelData/training_data_preprocessed/"
results_directory = "D:/Github/dungeonCrawlerMgr/LevelData/results_RL/"

def load_preprocessed_maps(directory, device):
    maps = []
    for filename in os.listdir(directory):
        if filename.endswith(".json"):
            with open(os.path.join(directory, filename), 'r') as f:
                data = json.load(f)
                map_data = np.array(data["LevelTileArray"])
                #map_data = np.transpose(map_data, (2, 0, 1))
                map_tensor = torch.tensor(map_data, dtype=torch.float32).to(device)
                maps.append(map_tensor)
    return maps

# class CustomCNN(BaseFeaturesExtractor):
#     def __init__(self, observation_space, features_dim=128):
#         super(CustomCNN, self).__init__(observation_space, features_dim)
#         self.cnn = nn.Sequential(
#             nn.Conv2d(7, 16, kernel_size=3, stride=1, padding=1),
#             nn.ReLU(),
#             nn.Conv2d(16, 32, kernel_size=3, stride=1, padding=1),
#             nn.ReLU(),
#             nn.Flatten(),
#             nn.Linear(32 * 20 * 20, features_dim),
#             nn.ReLU(),
#         )
#
#     def forward(self, x):
#         print("CustomCNN - Input shape:", x.shape)  # Add this line
#         #x = x.permute(0, 3, 1, 2).to(device)
#         #print("CustomCNN - After permute:", x.shape)  # Add this line
#         return self.cnn(x.to(device))

class CustomCNN(BaseFeaturesExtractor):
    def __init__(self, observation_space, features_dim=128):
        super(CustomCNN, self).__init__(observation_space, features_dim)
        self.device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

        # Define layers separately
        self.conv1 = nn.Conv2d(7, 16, kernel_size=3, stride=1, padding=1).to(self.device)
        self.conv2 = nn.Conv2d(16, 32, kernel_size=3, stride=1, padding=1).to(self.device)
        self.fc = nn.Linear(32 * 20 * 20, features_dim).to(self.device)
        print(f"CustomCNN self.device: {self.device}")

    def forward(self, x):
        print("CustomCNN - Input shape:", x.shape)

        # Apply layers manually
        x = x.to(device)
        print("CustomCNN - After to.device:", x.shape)
        x = x.permute(0, 3, 1, 2)#.to(device)
        if x.is_cuda:
            print("Data is on GPU.")
        else:
            print("Data is NOT on GPU!!")
        print(f"Weight device: {self.conv1.weight.device}")
        print(f"divice variable is {device}")
        self.conv1.weight = self.conv1.weight.to('cuda')
        self.conv1.bias = self.conv1.bias.to('cuda')
        print(f"Weight device after change: {self.conv1.weight.device}")
        x = F.relu(self.conv1(x))
        print("CustomCNN - After conv1(x):", x.shape)
        x = F.relu(self.conv2(x))
        print("CustomCNN - After conv2(x):", x.shape)
        #x = x.view(x.size(0), -1)  # Flatten
        x = x.reshape(x.size(0), -1)  # Flatten
        print("CustomCNN - After x.view(x.size(0), -1):", x.shape)
        x = F.relu(self.fc(x))
        print("CustomCNN - End:", x.shape)

        return x

class PolicyWrapper(nn.Module):
    def __init__(self, policy):
        super().__init__()
        self.policy = policy

    def forward(self, x):
        x = x.permute(0, 3, 1, 2).to(self.policy.device)#.to(device)
        return self.policy(x)#.to(self.policy.device))

class DungeonCrawlerEnv(Env):
    def __init__(self, maps, device):
        super(DungeonCrawlerEnv, self).__init__()
        self.current_step = 0
        self.maps = maps
        self.current_map = None
        self.action_space = spaces.Discrete(7)  # 7 tile types
        self.observation_space = spaces.Box(low=0, high=1, shape=(20, 20, 7), dtype=np.float32)
        self.max_steps = MAX_STEPS
        self.device = device

    def reset(self):
        self.current_map = torch.tensor(choice(self.maps), dtype=torch.float32).to(self.device)
        print("Reset - Current map shape:", self.current_map.shape)  # Add this line
        self.current_step = 0
        return self.current_map.to('cpu').numpy()
        #return self.current_map.cpu().numpy()  # [channels, height, width]

    # Modify current_map based on action
    # Calculate reward based on rules
    # Check if done
    # return observation, reward, done, info
    def step(self, action):
        self.current_map = self.current_map.to(self.device)

        # NARROW REPRESENTATION
        #x, y = np.random.choice(range(20), size=2)
        #self.current_map[x][y] = np.eye(7)[action]

        # WIDE REPRESENTATION
        #x, y = np.random.choice(range(1, 19), size=2)   # Randomly choose a central tile
        x, y = torch.randint(1, 19, (2,)).tolist()
        for i in range(-1, 2):      # Apply the action to a 3x3 area centered around (x, y)
            for j in range(-1, 2):
                self.current_map[x + i][y + j] = torch.eye(7)[action]

        # Calculate rewards
        reward = 0
        wall_tiles = torch.sum(self.current_map[:, :, 1])
        total_tiles = torch.numel(self.current_map) / 7

        if 0.35 <= wall_tiles.item() / total_tiles <= 0.45:
            reward += 0.8  # correct_wall_percentage
        else:
            reward -= -0.2  # incorrect_wall_percentage

        if torch.all(self.current_map[0, :, 1]) and torch.all(self.current_map[-1, :, 1]) and torch.all(
                self.current_map[:, 0, 1]) and torch.all(self.current_map[:, -1, 1]):
            reward += 0.7  # all_wall_edges
        else:
            reward -= -0.7  # not_all_wall_edges

        if torch.sum(self.current_map[:, :, 2]) == 1:
            reward += 1  # one_start
        else:
            reward -= 1  # not_one_start

        if torch.sum(self.current_map[:, :, 3]) == 1:
            reward += 0.6  # one_end
        else:
            reward -= 0.6  # not_one_end

        # calculate steps
        self.current_step += 1
        done = self.current_step >= self.max_steps
        info = {}

        print("Step - Current map shape:", self.current_map.shape)
        #return self.current_map.cpu().numpy(), reward, done, info
        return self.current_map.to('cpu').numpy(), reward, done, info


def save_onnx_model(model, epoch, results_directory):
    model_path = os.path.join(results_directory, f"ppo_model_epoch{epoch}.onnx")
    # Extract the PyTorch policy network from the Stable Baselines3 model
    policy = model.policy.to('cpu')
    # Create a dummy input that matches the input dimensions expected by the model
    dummy_input = torch.randn(1, 20, 20, 7) # 1 batch, 7 tile types, 20x20 map
    # Export the policy network to ONNX format
    torch.onnx.export(policy, dummy_input, model_path, verbose=True, input_names=['input'], output_names=['output'])

def plot_loss(loss_history, filename):
    plt.figure(figsize=(10, 5))
    plt.title("Loss over Time")
    plt.plot(loss_history)
    plt.xlabel("Epoch")
    plt.ylabel("Loss")
    plt.savefig(filename)
    plt.close()

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
print(f"training with {device}...")
maps = load_preprocessed_maps(preprocessed_data_directory, device)
env = DungeonCrawlerEnv(maps, device)
policy_kwargs = dict(
    features_extractor_class=CustomCNN,
    features_extractor_kwargs=dict(features_dim=128)
)
model = PPO("CnnPolicy", env, policy_kwargs=policy_kwargs, verbose=1, device=device)   # Proximal Policy Optimization (PPO)
model.policy.to(device)

for epoch in range(EPOCHS):
    print(f"Epoch: {epoch}")
    model.learn(total_timesteps=2000)
    if epoch % SAVE_INTERVAL == 0:
        save_onnx_model(model, epoch, results_directory)
        #plot_loss()
