# Reinforcement learning using openai/gym

import os
import json
import torch
import torch.nn as nn
import torch.optim as optim
import numpy as np
import matplotlib.pyplot as plt
from torch.distributions import Categorical
from gym import Env, spaces
from stable_baselines3 import PPO
from stable_baselines3.common.torch_layers import BaseFeaturesExtractor
from stable_baselines3 import PPO
from random import choice
import torch.nn.functional as F


# Hyperparameters
EPOCHS = 50
SAVE_INTERVAL = 5
LEARNING_RATE = 0.0005
GAMMA = 0.99
GAE_LAMBDA = 0.95
PPO_EPOCHS = 4
MINI_BATCH_SIZE = 64
PPO_CLIP = 0.2
TARGET_KL = 0.01
MAX_STEPS = 100

# Directories
preprocessed_data_directory = "D:/Github/dungeonCrawlerMgr/LevelData/training_data_preprocessed/"
results_directory = "D:/Github/dungeonCrawlerMgr/LevelData/results_RL/"

def load_preprocessed_maps(directory):
    maps = []
    for filename in os.listdir(directory):
        if filename.endswith(".json"):
            with open(os.path.join(directory, filename), 'r') as f:
                data = json.load(f)
                map_data = np.array(data["LevelTileArray"])
                #map_data = np.transpose(map_data, (2, 0, 1))
                maps.append(np.array(map_data))
    return maps

# class CustomCNN(BaseFeaturesExtractor):
#     def __init__(self, observation_space, features_dim=128):
#         super(CustomCNN, self).__init__(observation_space, features_dim)
#         self.cnn = nn.Sequential(
#             nn.Conv2d(7, 16, kernel_size=3, stride=1, padding=1),
#             nn.ReLU(),
#             nn.Conv2d(16, 32, kernel_size=3, stride=1, padding=1),
#             nn.ReLU(),
#             nn.Flatten(),
#             nn.Linear(32 * 20 * 20, features_dim),
#             nn.ReLU(),
#         )
#
#     def forward(self, x):
#         print("CustomCNN - Input shape:", x.shape)  # Add this line
#         #x = x.permute(0, 3, 1, 2).to(device)
#         #print("CustomCNN - After permute:", x.shape)  # Add this line
#         return self.cnn(x.to(device))

class CustomCNN(BaseFeaturesExtractor):
    def __init__(self, observation_space, features_dim=128):
        super(CustomCNN, self).__init__(observation_space, features_dim)

        # Define layers separately
        self.conv1 = nn.Conv2d(7, 16, kernel_size=3, stride=1, padding=1)
        self.conv2 = nn.Conv2d(16, 32, kernel_size=3, stride=1, padding=1)
        self.fc = nn.Linear(32 * 20 * 20, features_dim)

    def forward(self, x):
        print("CustomCNN - Input shape:", x.shape)

        # Apply layers manually
        x = x.to(device)
        print("CustomCNN - After to.device:", x.shape)
        x = F.relu(self.conv1(x))
        print("CustomCNN - After conv1(x):", x.shape)
        x = F.relu(self.conv2(x))
        print("CustomCNN - After conv2(x):", x.shape)
        x = x.view(x.size(0), -1)  # Flatten
        print("CustomCNN - After x.view(x.size(0), -1):", x.shape)
        x = F.relu(self.fc(x))
        print("CustomCNN - End:", x.shape)

        return x

class PolicyWrapper(nn.Module):
    def __init__(self, policy):
        super().__init__()
        self.policy = policy

    def forward(self, x):
        x = x.permute(0, 3, 1, 2).to(device)
        return self.policy(x.to(self.policy.device))

class DungeonCrawlerEnv(Env):
    def __init__(self, maps):
        super(DungeonCrawlerEnv, self).__init__()
        self.current_step = 0
        self.maps = maps
        self.current_map = None
        self.action_space = spaces.Discrete(7)  # 7 tile types
        self.observation_space = spaces.Box(low=0, high=1, shape=(20, 20, 7), dtype=np.float32)
        self.max_steps = MAX_STEPS

    def reset(self):
        self.current_map = torch.tensor(choice(self.maps), device=device, dtype=torch.float32)
        print("Reset - Current map shape:", self.current_map.shape)  # Add this line
        self.current_step = 0
        return self.current_map.cpu().numpy()  # [channels, height, width]

    # Modify current_map based on action
    # Calculate reward based on rules
    # Check if done
    # return observation, reward, done, info
    def step(self, action):

        # NARROW REPRESENTATION
        #x, y = np.random.choice(range(20), size=2)
        #self.current_map[x][y] = np.eye(7)[action]

        # WIDE REPRESENTATION
        x, y = np.random.choice(range(1, 19), size=2)   # Randomly choose a central tile
        for i in range(-1, 2):      # Apply the action to a 3x3 area centered around (x, y)
            for j in range(-1, 2):
                self.current_map[x + i][y + j] = torch.eye(7).to(device)[action]

        # Calculate rewards
        reward = 0
        wall_tiles = torch.sum(self.current_map[:, :, 1])
        total_tiles = torch.numel(self.current_map) / 7

        if 0.35 <= wall_tiles.item() / total_tiles <= 0.45:
            reward += 0.8  # correct_wall_percentage
        else:
            reward -= -0.2  # incorrect_wall_percentage

        if torch.all(self.current_map[0, :, 1]) and torch.all(self.current_map[-1, :, 1]) and torch.all(
                self.current_map[:, 0, 1]) and torch.all(self.current_map[:, -1, 1]):
            reward += 0.7  # all_wall_edges
        else:
            reward -= -0.7  # not_all_wall_edges

        if torch.sum(self.current_map[:, :, 2]) == 1:
            reward += 1  # one_start
        else:
            reward -= 1  # not_one_start

        if torch.sum(self.current_map[:, :, 3]) == 1:
            reward += 0.6  # one_end
        else:
            reward -= 0.6  # not_one_end

        # calculate steps
        self.current_step += 1
        done = self.current_step >= self.max_steps
        info = {}

        print("Step - Current map shape:", self.current_map.shape)  # Add this line
        return self.current_map.cpu().numpy(), reward, done, info


def save_onnx_model(model, epoch, results_directory):
    model_path = os.path.join(results_directory, f"ppo_model_epoch{epoch}.onnx")
    # Extract the PyTorch policy network from the Stable Baselines3 model
    policy = model.policy.to('cpu')
    # Create a dummy input that matches the input dimensions expected by the model
    dummy_input = torch.randn(1, 7, 20, 20)  # 1 batch, 7 tile types, 20x20 map
    # Export the policy network to ONNX format
    torch.onnx.export(policy, dummy_input, model_path, verbose=True, input_names=['input'], output_names=['output'])

maps = load_preprocessed_maps(preprocessed_data_directory)
env = DungeonCrawlerEnv(maps)
policy_kwargs = dict(
    features_extractor_class=CustomCNN,
    features_extractor_kwargs=dict(features_dim=128)
)
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
print(f"training with {device}...")
model = PPO("CnnPolicy", env, policy_kwargs=policy_kwargs, verbose=1, device=device)   # Proximal Policy Optimization (PPO)
for epoch in range(EPOCHS):
    print(f"Epoch: {epoch}")
    model.learn(total_timesteps=2000)
    if epoch % SAVE_INTERVAL == 0:
        save_onnx_model(model, epoch, results_directory)






# import os
# import json
# import torch
# import torch.nn as nn
# import torch.optim as optim
# import numpy as np
# import matplotlib.pyplot as plt
# from torch.distributions import Categorical
# from gym import Env, spaces
# from stable_baselines3 import PPO
# from stable_baselines3.common.torch_layers import BaseFeaturesExtractor
# from stable_baselines3 import PPO
# from random import choice
#
# # Hyperparameters
# EPOCHS = 50
# SAVE_INTERVAL = 1
# LEARNING_RATE = 0.0005
# GAMMA = 0.99
# GAE_LAMBDA = 0.95
# PPO_EPOCHS = 4
# MINI_BATCH_SIZE = 64
# PPO_CLIP = 0.2
# TARGET_KL = 0.01
# MAX_STEPS = 100
#
# # Directories
# preprocessed_data_directory = "D:/Github/dungeonCrawlerMgr/LevelData/training_data_preprocessed/"
# results_directory = "D:/Github/dungeonCrawlerMgr/LevelData/results_RL/"
#
# def load_preprocessed_maps(directory):
#     maps = []
#     for filename in os.listdir(directory):
#         if filename.endswith(".json"):
#             with open(os.path.join(directory, filename), 'r') as f:
#                 data = json.load(f)
#                 maps.append(np.array(data["LevelTileArray"]))
#     return maps
#
# class CustomCNN(BaseFeaturesExtractor):
#     def __init__(self, observation_space, features_dim=128):
#         super(CustomCNN, self).__init__(observation_space, features_dim)
#         self.cnn = nn.Sequential(
#             nn.Conv2d(7, 16, kernel_size=3, stride=1, padding=1),
#             nn.ReLU(),
#             nn.Conv2d(16, 32, kernel_size=3, stride=1, padding=1),
#             nn.ReLU(),
#             nn.Flatten(),
#             nn.Linear(32 * 20 * 20, features_dim),
#             nn.ReLU(),
#         )
#
#     def forward(self, x):
#         # Permute dimensions to [batch_size, channels, height, width]
#         x = x.permute(0, 3, 1, 2).to(self.cnn[0].weight.device)
#         return self.cnn(x)
#
# class PolicyWrapper(nn.Module):
#     def __init__(self, policy):
#         super().__init__()
#         self.policy = policy
#
#     def forward(self, x):
#         x = x.permute(0, 3, 1, 2).to(self.policy.device)  # Change shape from (N, H, W, C) to (N, C, H, W)
#         return self.policy(x)
#
# class DungeonCrawlerEnv(Env):
#     def __init__(self, maps):
#         super(DungeonCrawlerEnv, self).__init__()
#         self.current_step = 0
#         self.maps = maps
#         self.current_map = None
#         self.action_space = spaces.Discrete(7)  # 7 tile types
#         self.observation_space = spaces.Box(low=0, high=1, shape=(20, 20, 7), dtype=np.float32)
#         self.max_steps = MAX_STEPS
#
#     def reset(self):
#         #self.current_map = choice(self.maps)#np.random.choice(self.maps)
#         #self.current_map = torch.tensor(self.current_map).to(device)
#         self.current_map = torch.tensor(choice(self.maps), device=device, dtype=torch.float32)
#         self.current_step = 0
#         return self.current_map.cpu().numpy()
#
#     # Modify current_map based on action
#     # Calculate reward based on rules
#     # Check if done
#     # return observation, reward, done, info
#     def step(self, action):
#
#         # NARROW REPRESENTATION
#         #x, y = np.random.choice(range(20), size=2)
#         #self.current_map[x][y] = np.eye(7)[action]
#
#         # WIDE REPRESENTATION
#         x, y = np.random.choice(range(1, 19), size=2)   # Randomly choose a central tile
#         for i in range(-1, 2):      # Apply the action to a 3x3 area centered around (x, y)
#             for j in range(-1, 2):
#                 #self.current_map[x + i][y + j] = np.eye(7)[action]
#                 self.current_map[x + i][y + j] = torch.eye(7, device=device, dtype=torch.float32)[action]
#
#         # Calculate rewards
#         reward = 0
#         wall_tiles = torch.sum(self.current_map[:, :, 1])
#         total_tiles = self.current_map.numel() / 7
#
#         if 0.35 <= wall_tiles.item() / total_tiles <= 0.45:
#             reward += 0.8  # correct_wall_percentage
#         else:
#             reward -= 0.2  # incorrect_wall_percentage
#
#         if torch.all(self.current_map[0, :, 1]) and torch.all(self.current_map[-1, :, 1]) and torch.all(
#                 self.current_map[:, 0, 1]) and torch.all(self.current_map[:, -1, 1]):
#             reward += 0.7  # all_wall_edges
#         else:
#             reward -= 0.7  # not_all_wall_edges
#
#         if torch.sum(self.current_map[:, :, 2]) == 1:
#             reward += 1  # one_start
#         else:
#             reward -= 1  # not_one_start
#
#         if torch.sum(self.current_map[:, :, 3]) == 1:
#             reward += 0.6  # one_end
#         else:
#             reward -= 0.6  # not_one_end
#
#         # calculate steps
#         self.current_step += 1
#         done = self.current_step >= self.max_steps
#         info = {}
#
#         return self.current_map.cpu().numpy(), reward, done, info
#
#
# def save_onnx_model(model, epoch, results_directory):
#     model_path = os.path.join(results_directory, f"ppo_model_epoch{epoch}.onnx")
#     # Extract the PyTorch policy network from the Stable Baselines3 model
#     policy = model.policy.to('cpu')
#     # Create a dummy input that matches the input dimensions expected by the model
#     dummy_input = (torch.randn(1, 20, 20, 7)).to('cpu')  # 1 batch, 7 tile types, 20x20 map
#     # Export the policy network to ONNX format
#     torch.onnx.export(policy, dummy_input, model_path, verbose=True, input_names=['input'], output_names=['output'])
#
# maps = load_preprocessed_maps(preprocessed_data_directory)
# env = DungeonCrawlerEnv(maps)
# policy_kwargs = dict(
#     features_extractor_class=CustomCNN,
#     features_extractor_kwargs=dict(features_dim=128)
# )
# device = "cuda" if torch.cuda.is_available() else "cpu"
# print(f"training with {device}...")
# model = PPO("CnnPolicy", env, policy_kwargs=policy_kwargs, verbose=1, device=device)   # Proximal Policy Optimization (PPO)
# for epoch in range(EPOCHS):
#     print(f"Epoch: {epoch}")
#     model.learn(total_timesteps=2000)
#     if epoch % SAVE_INTERVAL == 0:
#         save_onnx_model(model, epoch, results_directory)
#         #model.save(os.path.join(results_directory, f"ppo_model_epoch{epoch}.onnx"))




# TURTLE REPRESENTATION
# class DungeonCrawlerEnv(Env):
#     def __init__(self, maps):
#         super(DungeonCrawlerEnv, self).__init__()
#         self.maps = maps
#         self.current_map = None
#         self.action_space = spaces.Discrete(7)  # 7 tile types
#         self.observation_space = spaces.Box(low=0, high=1, shape=(20, 20, 7), dtype=np.float32)
#         self.turtle_pos = [0, 0]  # Starting position of the turtle
#
#     def reset(self):
#         self.current_map = np.random.choice(self.maps)
#         return self.current_map
#
#     def step(self, action):
#         # Decode action
#         # 10 actions: {0: move up, 1: move down, 2: move left, 3: move right, 4: place type 0, 5: place type 1,
#         # 6: place type 2, 7: place type 3, 8: place type 4, 9: place type 5, 10: place type 6}
#         if action == 0:   # Move Up
#             self.turtle_pos[0] = max(0, self.turtle_pos[0] - 1)
#         elif action == 1: # Move Down
#             self.turtle_pos[0] = min(19, self.turtle_pos[0] + 1)
#         elif action == 2: # Move Left
#             self.turtle_pos[1] = max(0, self.turtle_pos[1] - 1)
#         elif action == 3: # Move Right
#             self.turtle_pos[1] = min(19, self.turtle_pos[1] + 1)
#         else:             # Place Tile
#             self.current_map[self.turtle_pos[0]][self.turtle_pos[1]] = np.eye(7)[action - 4]
#
#         # Calculate rewards
#         reward = 0
#         wall_tiles = np.sum(self.current_map[:, :, 1])
#         total_tiles = self.current_map.size / 7
#
#         if 0.35 <= wall_tiles / total_tiles <= 0.45:
#             reward += 0.8
#         if np.all(self.current_map[0, :, 1]) and np.all(self.current_map[-1, :, 1]) and np.all(
#                 self.current_map[:, 0, 1]) and np.all(self.current_map[:, -1, 1]):
#             reward += 0.7
#         if np.sum(self.current_map[:, :, 2]) == 1:
#             reward += 1
#         if np.sum(self.current_map[:, :, 3]) == 1:
#             reward += 0.6
#
#         # For simplicity, we'll say that an episode is done after a single step
#         done = True
#         info = {}
#
#         return self.current_map, reward, done, info