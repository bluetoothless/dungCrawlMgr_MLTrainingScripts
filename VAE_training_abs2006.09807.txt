#2006.09807
import os
import json
import torch
import torch.nn as nn
import torch.nn.functional as F
import torch.optim as optim
from torch.utils import data as data_utils
import torch.onnx
import onnx
import onnxruntime as ort
import matplotlib.pyplot as plt

preprocessed_data_directory = "D:/Github/dungeonCrawlerMgr/LevelData/training_data_preprocessed/"
results_directory = "D:/Github/dungeonCrawlerMgr/LevelData/results_VAE/"
validation_data_directory = "D:/Github/dungeonCrawlerMgr/LevelData/validation_data_preprocessed/"
train_losses = []
val_losses = []

# HYPERPARAMETERS
nr_of_epochs = 5000
save_model_step = 100  # save model after every x epochs
latent_space_dimensions = 32
batch_size = 32
learning_rate = 0.001
step_size = 750
gamma = 0.9     # After [step_size] epochs, [learning_rate] = [learning_rate] * [gamma]
# (learning_rate = 0.0001), batch_size    128 -> w ep. 230 Loss = 534, Vali_Loss = 551;    64 -> w ep. 230 Loss = 495, Vali_Loss = 541;     32 -> w ep. 230 Loss = 442, Vali_Loss = 531;
# (learning_rate = 0.0005), batch_size    128 -> w ep. 230 Loss = 395, Vali_Loss = 540;    64 -> w ep. 230 Loss = 267, Vali_Loss = 619;     32 -> w ep. 230 Loss = 213, Vali_Loss = 723;
# (learning_rate = 0.001), batch_size    128 -> w ep. 230 Loss = 250, Vali_Loss = 671;    64 -> w ep. 230 Loss = 173, Vali_Loss = 807;


#reconstruction accuracy

# Data loading
def load_preprocessed_maps(directory):
    maps = []
    for filename in os.listdir(directory):
        if filename.endswith(".json"):
            with open(os.path.join(directory, filename), 'r') as f:
                data = json.load(f)
                maps.append(data["LevelTileArray"])
    return maps

def load_validation_data(directory):
    maps = []
    for filename in os.listdir(directory):
        if filename.endswith(".json"):
            with open(os.path.join(directory, filename), 'r') as f:
                data = json.load(f)
                maps.append(data["LevelTileArray"])
    return maps

# Defining VAE Architecture
class VAE(nn.Module):
    def __init__(self):
        super(VAE, self).__init__()

        self.enc1 = nn.Conv2d(7, 16, kernel_size=3, stride=2, padding=1)
        self.enc2 = nn.Conv2d(16, 32, kernel_size=3, stride=2, padding=1)
        self.enc_bn1 = nn.BatchNorm2d(16)
        self.enc_bn2 = nn.BatchNorm2d(32)
        self.fc1 = nn.Linear(32 * 5 * 5, latent_space_dimensions)
        self.fc2 = nn.Linear(32 * 5 * 5, latent_space_dimensions)

        self.fc3 = nn.Linear(latent_space_dimensions, 32 * 5 * 5)
        self.dec1 = nn.ConvTranspose2d(32, 16, kernel_size=3, stride=2, padding=1, output_padding=1)
        self.dec2 = nn.ConvTranspose2d(16, 7, kernel_size=3, stride=2, padding=1, output_padding=1)
        self.dec_bn1 = nn.BatchNorm2d(16)
        self.dec_bn2 = nn.BatchNorm2d(7)

    def encode(self, x):
        x = F.leaky_relu(self.enc_bn1(self.enc1(x)))
        x = F.leaky_relu(self.enc_bn2(self.enc2(x)))
        x = x.view(x.size(0), -1)
        mu = self.fc1(x)
        log_var = self.fc2(x)
        return mu, log_var

    def reparameterize(self, mu, log_var):
        std = torch.exp(0.5*log_var)
        eps = torch.randn_like(std)
        return mu + eps*std

    def decode(self, z):
        z = self.fc3(z)
        z = z.view(z.size(0), 32, 5, 5)
        z = F.relu(self.dec_bn1(self.dec1(z)))
        z = torch.sigmoid(self.dec_bn2(self.dec2(z)))
        return z

    def forward(self, x):
        mu, log_var = self.encode(x)
        z = self.reparameterize(mu, log_var)
        return self.decode(z), mu, log_var

# Loss Function
def loss_function(recon_x, x, mu, logvar):
    BCE = nn.functional.binary_cross_entropy(recon_x, x, reduction='sum')
    KLD = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())
    return BCE + KLD

def train_vae(model, dataloader, optimizer):
    model.train()
    train_loss = 0
    for batch_idx, data in enumerate(dataloader):
        data = data.to(device)
        optimizer.zero_grad()
        recon_batch, mu, logvar = model(data)
        loss = loss_function(recon_batch, data, mu, logvar)
        loss.backward()
        train_loss += loss.item()
        optimizer.step()
    avg_train_loss = train_loss / len(dataloader.dataset)
    train_losses.append(avg_train_loss)
    return avg_train_loss

# Validation
def validate_vae(model, dataloader):
    model.eval()
    val_loss = 0
    with torch.no_grad():
        for i, data in enumerate(dataloader):
            data = data.to(device)
            recon, mu, logvar = model(data)
            val_loss += loss_function(recon, data, mu, logvar).item()
    avg_val_loss = val_loss / len(dataloader.dataset)
    val_losses.append(avg_val_loss)
    return avg_val_loss

def save_model(model):
    #dummy_input = torch.randn(1, 2800).to(device)  # Assuming the input size to the model is [1, 2800]
    next_number = get_next_number()
    torch.save(model.state_dict(), f"{results_directory}model_{next_number}.pt")
    dummy_input = torch.randn([1, 7, 20, 20]).to(device)
    file_path = results_directory + f"VAE_model_{next_number}.onnx"
    torch.onnx.export(model, dummy_input, file_path, verbose=True)

def save_plot(train_losses, val_losses, number):
    plt.figure()
    plt.plot(val_losses, label="Validation Loss", color='red')
    plt.plot(train_losses, label="Training Loss", color='blue')
    plt.xlabel('Epoch')
    plt.ylabel('Loss')
    plt.legend()
    plt.savefig(results_directory + f"VAE_plot_{number}.png")
    plt.close()

def get_next_number():
    files = os.listdir(results_directory)
    max_num = -1
    for file in files:
        if "VAE_model_" in file:
            try:
                num = int(file.split("_")[-1].split(".")[0])
                max_num = max(max_num, num)
            except ValueError:
                pass
    return max_num + 1

choice = input("Do you want to (Enter 1 or 2): \n(1) Start new training \n(2) Continue previous training\n")
if choice == "1":
    model = VAE()
elif choice == "2":
    model_path = results_directory + f"model_{get_next_number() - 1}.pt"
    model = VAE()
    model.load_state_dict(torch.load(model_path))
else:
    print("Invalid choice.")
    exit()

if torch.cuda.is_available():
    print("Training with GPU...")
    device = torch.device("cuda")
    model.to(device)
else:
    device = torch.device("cpu")

optimizer = optim.Adam(model.parameters(), lr=learning_rate)
maps = torch.FloatTensor(load_preprocessed_maps(preprocessed_data_directory))
maps = maps.permute(0, 3, 1, 2)
validation_maps = torch.FloatTensor(load_validation_data(validation_data_directory))
validation_maps = validation_maps.permute(0, 3, 1, 2)

train_dataloader = data_utils.DataLoader(maps, batch_size=batch_size, shuffle=True)
val_dataloader = data_utils.DataLoader(validation_maps, batch_size=batch_size)

for epoch in range(1, nr_of_epochs + 1):
    train_loss = train_vae(model, train_dataloader, optimizer)
    val_loss = validate_vae(model, val_dataloader)
    if epoch % step_size == 0:
        learning_rate = learning_rate * gamma
    print(f"Epoch {epoch}, Train loss: {train_loss}, Val loss: {val_loss}, Learning rate: {learning_rate}")

    if epoch % save_model_step == 0 or epoch == 500 or epoch == 1000:
        save_model(model)
        save_plot(train_losses, val_losses, get_next_number() - 1)