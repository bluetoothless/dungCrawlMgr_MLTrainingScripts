#2102.12463	9x9
import os
import json
import torch
import torch.nn as nn
import torch.optim as optim
import matplotlib.pyplot as plt
import numpy as np
import optuna
from torch.utils.data import DataLoader, TensorDataset

# Hyperparameters
LATENT_DIM = 32
SAVE_STEP = 200
NUMBER_OF_EPOCHS = 10001 #30 #10001

LEARNING_RATE = 0.0008 #0.001
LR_DECAY = 0.00004 #0.001
LR_DECAY_EPOCH = 4500 #2500
BATCH_SIZE = 32 #64
#annealing - introducing a weight for the KLD loss and gradually increasing it during training
KLD_WEIGHT = 0.01
KLD_MAX = 1.798 #1        # Maximum weight for KLD loss
KLD_ANNEAL_RATE = 0.00001 #0.0001  # Rate at which to increase KLD weight

# Best trial:
#   Value: 73.4289176825321
#   Params:
#     learning_rate: 0.0007890264827785901
#     lr_decay: 3.876965025956844e-05
#     lr_decay_epoch: 4500
#     batch_size: 32
#     kld_weight: 0.01117096944547276
#     kld_max: 1.7984552082226901
#     kld_anneal_rate: 1.0690814630397052e-05

# Data Directories
preprocessed_data_directory = "D:/Github/dungeonCrawlerMgr/LevelData/training_data_preprocessed_small_9x9/"
validation_data_directory = "D:/Github/dungeonCrawlerMgr/LevelData/validation_data_preprocessed_small_9x9/"
results_directory = "D:/Github/dungeonCrawlerMgr/LevelData/results_VAE/"

# Load Preprocessed Maps
def load_preprocessed_maps(directory):
    maps = []
    for filename in os.listdir(directory):
        if filename.endswith(".json"):
            with open(os.path.join(directory, filename), 'r') as f:
                data = json.load(f)
                maps.append(data["LevelTileArray"])
    return np.array(maps)

# Save Model
def save_model(model, device):
    next_number = get_next_number()
    torch.save(model.state_dict(), f"{results_directory}model_small9x9_2102.12463_{next_number}.pt")
    dummy_input = torch.randn([1, 7, 9, 9]).to(device)
    file_path = results_directory + f"VAE_model_small9x9_2102.12463_{next_number}.onnx"
    torch.onnx.export(model, dummy_input, file_path, verbose=True)

# Get Next Number for Model Saving
def get_next_number():
    files = os.listdir(results_directory)
    max_num = -1
    for file in files:
        if "VAE_model_" in file:
            try:
                num = int(file.split("_")[-1].split(".")[0])
                max_num = max(max_num, num)
            except ValueError:
                pass
    return max_num + 1

# Save Plot
def save_plot(train_losses, val_losses, number):
    plt.figure()
    plt.plot(val_losses, label="Validation Loss", color='red')
    plt.plot(train_losses, label="Training Loss", color='blue')
    plt.xlabel('Epoch')
    plt.ylabel('Loss')
    plt.legend()
    plt.savefig(results_directory + f"VAE_plot_small9x9_2102.12463_{number}.png")
    plt.close()

    # Define VAE
class VAE(nn.Module):
    def __init__(self):
        super(VAE, self).__init__()

        # Encoder
        self.enc1 = nn.Linear(7 * 9 * 9, 512)
        self.enc2 = nn.Linear(512, 256)
        self.enc3 = nn.Linear(256, 128)
        self.enc4 = nn.Linear(128, LATENT_DIM * 2)

        # Decoder
        self.dec1 = nn.Linear(LATENT_DIM, 128)
        self.dec2 = nn.Linear(128, 256)
        self.dec3 = nn.Linear(256, 512)
        self.dec4 = nn.Linear(512, 7 * 9 * 9)

        self.relu = nn.ReLU()

    def reparameterize(self, mu, logvar):
        std = torch.exp(0.5 * logvar)
        eps = torch.randn_like(std)
        return mu + eps * std

    def forward(self, x):
        x = x.view(x.size(0), -1)
        h = self.relu(self.enc1(x))
        h = self.relu(self.enc2(h))
        h = self.relu(self.enc3(h))
        h = self.enc4(h)

        mu, logvar = torch.chunk(h, 2, dim=1)
        z = self.reparameterize(mu, logvar)

        h = self.relu(self.dec1(z))
        h = self.relu(self.dec2(h))
        h = self.relu(self.dec3(h))
        h = self.dec4(h)

        return h.view(h.size(0), 7, 9, 9), mu, logvar

# Loss Function
def vae_loss(recon_x, x, mu, logvar, kld_weight):
    BCE = nn.functional.binary_cross_entropy_with_logits(recon_x, x, reduction='sum')
    KLD = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())
    return BCE + kld_weight * KLD, BCE, kld_weight * KLD

def objective(trial):
    # Hyperparameters to be optimized by Optuna
    LEARNING_RATE = trial.suggest_loguniform('learning_rate', 0.00001, 0.1)
    LR_DECAY = trial.suggest_loguniform('lr_decay', 0.00001, 0.01)
    LR_DECAY_EPOCH = trial.suggest_int('lr_decay_epoch', 1000, 5000, step=500)
    BATCH_SIZE = trial.suggest_categorical('batch_size', [32, 64, 128, 256, 512])
    KLD_WEIGHT = trial.suggest_uniform('kld_weight', 0.01, 1.0)
    KLD_MAX = trial.suggest_uniform('kld_max', 0.5, 2.0)
    KLD_ANNEAL_RATE = trial.suggest_loguniform('kld_anneal_rate', 0.00001, 0.01)

    # Load Data
    train_data = load_preprocessed_maps(preprocessed_data_directory)
    val_data = load_preprocessed_maps(validation_data_directory)
    train_data = torch.tensor(train_data, dtype=torch.float32)
    val_data = torch.tensor(val_data, dtype=torch.float32)
    train_loader = DataLoader(TensorDataset(train_data), batch_size=BATCH_SIZE, shuffle=True)
    val_loader = DataLoader(TensorDataset(val_data), batch_size=BATCH_SIZE)

    # Initialize Model and Optimizer
    model = VAE()
    optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)

    # Training Loop
    train_losses = []
    val_losses = []
    for epoch in range(1, NUMBER_OF_EPOCHS):  # Adjust the range based on your needs
        model.train()
        train_loss = 0
        loss_BCE = 0
        loss_KLD = 0
        KLD_WEIGHT = min(KLD_WEIGHT + KLD_ANNEAL_RATE, KLD_MAX)
        for batch_idx, (data,) in enumerate(train_loader):
            optimizer.zero_grad()
            recon_batch, mu, logvar = model(data)
            loss, loss_BCE_tmp, loss_KLD_tmp = vae_loss(recon_batch, data.permute(0, 3, 1, 2), mu, logvar, KLD_WEIGHT)
            loss.backward()
            train_loss += loss.item()
            loss_BCE += loss_BCE_tmp.item()
            loss_KLD += loss_KLD_tmp.item()
            optimizer.step()

        train_loss /= len(train_loader.dataset)
        loss_BCE /= len(train_loader.dataset)
        loss_KLD /= len(train_loader.dataset)
        train_losses.append(train_loss)

        # Validation
        model.eval()
        val_loss = 0
        with torch.no_grad():
            for i, (data,) in enumerate(val_loader):
                recon_batch, mu, logvar = model(data)
                val_loss_tmp, val_BCE, val_KLD = vae_loss(recon_batch, data.permute(0, 3, 1, 2), mu, logvar, KLD_WEIGHT)
                val_loss += val_loss_tmp.item()

        val_loss /= len(val_loader.dataset)
        val_losses.append(val_loss)

        current_lr = optimizer.param_groups[0]['lr']
        print(
            f"Epoch {epoch} - Train Loss: {train_loss:.4f} (BCE Loss: {loss_BCE:.4f} + KLD loss: {loss_KLD:.4f}), Validation Loss: {val_loss:.4f}, Learning Rate: {current_lr:.4f}")

        # Learning rate decay
        if epoch % LR_DECAY_EPOCH == 0:
            for param_group in optimizer.param_groups:
                param_group['lr'] *= (1 - LR_DECAY)

    return train_losses[-1]

# Main Function
if __name__ == "__main__":
    # Finding the best hyperparameters
    # study = optuna.create_study(direction='minimize', sampler=optuna.samplers.TPESampler())
    # study.optimize(objective, n_trials=100)  # Adjust the number of trials
    # print("Best trial:")
    # trial = study.best_trial
    # print(f"  Value: {trial.value}")
    # print("  Params: ")
    # for key, value in trial.params.items():
    #     print(f"    {key}: {value}")

    # Load Data
    train_data = load_preprocessed_maps(preprocessed_data_directory)
    val_data = load_preprocessed_maps(validation_data_directory)

    train_data = torch.tensor(train_data, dtype=torch.float32)
    val_data = torch.tensor(val_data, dtype=torch.float32)

    train_loader = DataLoader(TensorDataset(train_data), batch_size=BATCH_SIZE, shuffle=True)
    val_loader = DataLoader(TensorDataset(val_data), batch_size=BATCH_SIZE)

    # Initialize Model and Optimizer
    model = VAE()
    optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)

    # Training Loop
    train_losses = []
    val_losses = []
    for epoch in range(1, 10001):  # Adjust the range based on your needs
        model.train()
        train_loss = 0
        loss_BCE = 0
        loss_KLD = 0
        KLD_WEIGHT = min(KLD_WEIGHT + KLD_ANNEAL_RATE, KLD_MAX)
        for batch_idx, (data,) in enumerate(train_loader):
            optimizer.zero_grad()
            recon_batch, mu, logvar = model(data)
            loss, loss_BCE_tmp, loss_KLD_tmp = vae_loss(recon_batch, data.permute(0, 3, 1, 2), mu, logvar, KLD_WEIGHT)
            loss.backward()
            train_loss += loss.item()
            loss_BCE += loss_BCE_tmp.item()
            loss_KLD += loss_KLD_tmp.item()
            optimizer.step()

        train_loss /= len(train_loader.dataset)
        loss_BCE /= len(train_loader.dataset)
        loss_KLD /= len(train_loader.dataset)
        train_losses.append(train_loss)

        # Validation
        model.eval()
        val_loss = 0
        with torch.no_grad():
            for i, (data,) in enumerate(val_loader):
                recon_batch, mu, logvar = model(data)
                val_loss_tmp, val_BCE, val_KLD = vae_loss(recon_batch, data.permute(0, 3, 1, 2), mu, logvar, KLD_WEIGHT)
                val_loss += val_loss_tmp.item()

        val_loss /= len(val_loader.dataset)
        val_losses.append(val_loss)

        current_lr = optimizer.param_groups[0]['lr']
        print(f"Epoch {epoch} - Train Loss: {train_loss:.4f} (BCE Loss: {loss_BCE:.4f} + KLD loss: {loss_KLD:.4f}), Validation Loss: {val_loss:.4f}, Learning Rate: {current_lr:.4f}")

        # Save model and plots
        if epoch % SAVE_STEP == 0:
            save_model(model, torch.device("cpu"))  # Change to "cuda" if using GPU
            save_plot(train_losses, val_losses, get_next_number())

        # Learning rate decay
        if epoch % LR_DECAY_EPOCH == 0:
            for param_group in optimizer.param_groups:
                param_group['lr'] *= (1 - LR_DECAY)